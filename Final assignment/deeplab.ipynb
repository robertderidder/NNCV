{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.optim import lr_scheduler\n",
    "from model import Model\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import Cityscapes, wrap_dataset_for_transforms_v2\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.transforms.v2 import (\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    Resize,\n",
    "    ToImage,\n",
    "    ToDtype,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomVerticalFlip,\n",
    ")\n",
    "\n",
    "# Mapping class IDs to train IDs\n",
    "id_to_trainid = {cls.id: cls.train_id for cls in Cityscapes.classes}\n",
    "def convert_to_train_id(label_img: torch.Tensor) -> torch.Tensor:\n",
    "    return label_img.apply_(lambda x: id_to_trainid[x])\n",
    "\n",
    "# Mapping train IDs to color\n",
    "train_id_to_color = {cls.train_id: cls.color for cls in Cityscapes.classes if cls.train_id != 255}\n",
    "train_id_to_color[255] = (0, 0, 0)  # Assign black to ignored labels\n",
    "\n",
    "def convert_train_id_to_color(prediction: torch.Tensor) -> torch.Tensor:\n",
    "    batch, _, height, width = prediction.shape\n",
    "    color_image = torch.zeros((batch, 3, height, width), dtype=torch.uint8)\n",
    "\n",
    "    for train_id, color in train_id_to_color.items():\n",
    "        mask = prediction[:, 0] == train_id\n",
    "\n",
    "        for i in range(3):\n",
    "            color_image[:, i][mask] = color[i]\n",
    "\n",
    "    return color_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducability\n",
    "# If you add other sources of randomness (NumPy, Random), \n",
    "# make sure to set their seeds as well\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transforms to apply to the data\n",
    "class PaintingByNumbersTransform:\n",
    "      def __init__(self, id_to_color=None):\n",
    "          self.id_to_color = id_to_color  # Dictionary mapping class IDs to colors\n",
    "  \n",
    "      def random_recolor(self, label_img):\n",
    "          \"\"\"Assigns random colors to segmentation labels.\"\"\"\n",
    "          h, w = label_img.shape[1:]\n",
    "          recolored = torch.zeros((3, h, w), dtype=torch.uint8)  # Create an empty RGB image\n",
    "                  \n",
    "          unique_labels = label_img.unique()\n",
    "          color_map = {label.item(): torch.randint(0, 256, (3,), dtype=torch.uint8) for label in unique_labels}\n",
    "\n",
    "          for label, color in color_map.items():\n",
    "              mask = (label_img[0] == label)  # label_img shape is [1, h, w]\n",
    "              for c in range(3):\n",
    "                  recolored[c][mask] = color[c]\n",
    "             \n",
    "          return recolored\n",
    "  \n",
    "      def __call__(self, img, target):\n",
    "          if torch.rand(1).item() > 0.5:\n",
    "              # Load the actual ground truth color image\n",
    "              gt_color = self.random_recolor(target)\n",
    "  \n",
    "              # Blend image and color segmentation map\n",
    "              alpha = torch.rand(1).item() * 0.29 + 0.7  # Random alpha between 0.7 and 0.99\n",
    "              blended_img = alpha * img + (1 - alpha) * gt_color.float() / 255.0\n",
    "              return blended_img, target\n",
    "          \n",
    "          return img, target  # If not applying transformation, return original\n",
    "\n",
    "\n",
    "transform = Compose([\n",
    "    ToImage(),\n",
    "    Resize((256, 256)),\n",
    "    ToDtype(torch.float32, scale=True),\n",
    "    Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    PaintingByNumbersTransform(),\n",
    "    RandomHorizontalFlip(p=0.5),\n",
    "    RandomVerticalFlip(p=0.5),\n",
    "])\n",
    "\n",
    "# Load the dataset and make a split for training and validation\n",
    "train_dataset = Cityscapes(\n",
    "    \"data/cityscapes\", \n",
    "    split=\"train\", \n",
    "    mode=\"fine\", \n",
    "    target_type=\"semantic\", \n",
    "    transforms=transform\n",
    ")\n",
    "valid_dataset = Cityscapes(\n",
    "    \"data/cityscapes\", \n",
    "    split=\"val\", \n",
    "    mode=\"fine\", \n",
    "    target_type=\"semantic\", \n",
    "    transforms=transform\n",
    ")\n",
    "\n",
    "train_dataset = wrap_dataset_for_transforms_v2(train_dataset)\n",
    "valid_dataset = wrap_dataset_for_transforms_v2(valid_dataset)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=64, \n",
    "    shuffle=True,\n",
    "    num_workers=10\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size=64, \n",
    "    shuffle=False,\n",
    "    num_workers=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=255)  # Ignore the void class\n",
    "# Define the optimizer\n",
    "lr1 = 0.01\n",
    "lr2 = 0.001\n",
    "\n",
    "model = Model().to(device)\n",
    "\n",
    "optimizer1 = AdamW([\n",
    "{\"params\": model.model.classifier.parameters(), \"lr\": lr1}  # Higher LR for classifier\n",
    "])\n",
    "\n",
    "optimizer2 = AdamW([\n",
    "{\"params\": model.model.backbone.parameters(), \"lr\": lr2}  # Lower LR for backbone\n",
    "])\n",
    "\n",
    "scheduler = lr_scheduler.StepLR(optimizer1, 2, gamma=0.5, last_epoch=-1)\n",
    "\n",
    "\n",
    "\n",
    "for param in model.model.backbone.parameters():\n",
    "    param.requires_grad = True  # Unfreeze the backbone\n",
    "    \n",
    "for param in model.model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0001/0010\n",
      "Epoch 0002/0010\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmodel(images)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     33\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 34\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     37\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "best_valid_loss = float('inf')\n",
    "current_best_model_path = None\n",
    "for epoch in range(10):\n",
    "    print(f\"Epoch {epoch+1:04}/{10:04}\")\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(train_dataloader):\n",
    "\n",
    "        labels = convert_to_train_id(labels)  # Convert class IDs to train IDs\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        labels = labels.long().squeeze(1)  # Remove channel dimension\n",
    "\n",
    "        optimizer1.zero_grad()\n",
    "        outputs = model.model(images)['out']\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer1.step()\n",
    "        \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        losses = []\n",
    "        for i, (images, labels) in enumerate(valid_dataloader):\n",
    "            labels = convert_to_train_id(labels)  # Convert class IDs to train IDs\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            labels = labels.long().squeeze(1)  # Remove channel dimension\n",
    "\n",
    "            outputs = model.model(images)['out']\n",
    "            loss = criterion(outputs, labels)\n",
    "            losses.append(loss.item())\n",
    "        \n",
    "            if i == 0:\n",
    "                predictions = outputs.softmax(1).argmax(1)\n",
    "\n",
    "                predictions = predictions.unsqueeze(1)\n",
    "                labels = labels.unsqueeze(1)\n",
    "\n",
    "                predictions = convert_train_id_to_color(predictions)\n",
    "                labels = convert_train_id_to_color(labels)\n",
    "\n",
    "                predictions_img = make_grid(predictions.cpu(), nrow=8)\n",
    "                labels_img = make_grid(labels.cpu(), nrow=8)\n",
    "\n",
    "                predictions_img = predictions_img.permute(1, 2, 0).numpy()\n",
    "                labels_img = labels_img.permute(1, 2, 0).numpy()\n",
    "\n",
    "        \n",
    "        valid_loss = sum(losses) / len(losses)\n",
    "       \n",
    "        # if valid_loss < best_valid_loss:\n",
    "        #     best_valid_loss = valid_loss\n",
    "        #     if current_best_model_path:\n",
    "        #         os.remove(current_best_model_path)\n",
    "        #     current_best_model_path = os.path.join(\n",
    "        #         output_dir, \n",
    "        #         f\"best_model-epoch={epoch:04}-val_loss={valid_loss:04}.pth\"\n",
    "        #     )\n",
    "        #     torch.save(model.state_dict(), current_best_model_path)\n",
    "    \n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"model_from_notebook.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()  # No dropout or batchnorm updates\n",
    "with torch.no_grad():\n",
    "    for images, labels in train_dataloader:\n",
    "        labels = convert_to_train_id(labels)\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        labels = labels.long().squeeze(1)\n",
    "        \n",
    "        outputs = model.model(images)['out']\n",
    "        break  # Exit after one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "data = torch.load('batch_dump.pt')\n",
    "images = data['images']\n",
    "labels = data['labels']\n",
    "outputs = data['outputs']\n",
    "#outputs = F.softmax(outputs,dim=1) #Shape [N,C,H,W]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dice = 0\n",
    "outputs = F.softmax(outputs,dim=1)\n",
    "eps = 1e-5\n",
    "for c in range(19):\n",
    "    outputtest=outputs[:,c,:,:]\n",
    "    target_mask = (labels == c).float()  # shape [H, W], values in {0.0, 1.0}\n",
    "\n",
    "    intersection = (outputtest*target_mask).sum()\n",
    "    union = target_mask.sum()+outputtest.sum()\n",
    "    dice += (2*intersection+eps)/(union+eps)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected index [64, 1, 4194304] to be no larger than self [64, 19, 65536] apart from dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 13\u001b[0m\n\u001b[0;32m     10\u001b[0m target1 \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mview(N, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m# (N, 1, *)\u001b[39;00m\n\u001b[0;32m     12\u001b[0m target_onehot \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(prediction1\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m---> 13\u001b[0m \u001b[43mtarget_onehot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (N, C, *)\u001b[39;00m\n\u001b[0;32m     15\u001b[0m intersection \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(prediction \u001b[38;5;241m*\u001b[39m target_onehot, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# (N, C)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m union \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(prediction\u001b[38;5;241m.\u001b[39mpow(\u001b[38;5;241m2\u001b[39m), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39msum(target_onehot, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)  \u001b[38;5;66;03m# (N, C)\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Expected index [64, 1, 4194304] to be no larger than self [64, 19, 65536] apart from dimension 1"
     ]
    }
   ],
   "source": [
    "N,C,H,W = outputs.shape # prediction: [N, C, H, W]\n",
    "prediction = F.softmax(outputs, dim=1) \n",
    "target = labels\n",
    "\n",
    "# Create a mask for valid pixels (not ignore_index)\n",
    "valid_mask = (target != 255).unsqueeze(1)  # [N, 1, H, W]\n",
    "target = target.masked_fill(valid_mask,0)\n",
    "\n",
    "prediction1 = prediction.view(N, C, -1) # (N, C, *)\n",
    "target1 = target.view(N, 1, -1) # (N, 1, *)\n",
    "\n",
    "target_onehot = torch.zeros(prediction1.size())\n",
    "target_onehot.scatter_(1, target1, 1)  # (N, C, *)\n",
    "\n",
    "intersection = torch.sum(prediction * target_onehot, dim=2)  # (N, C)\n",
    "union = torch.sum(prediction.pow(2), dim=2) + torch.sum(target_onehot, dim=2)  # (N, C)\n",
    "## p^2 + t^2 >= 2*p*t, target_onehot^2 == target_onehot\n",
    "dice_coef = (2 * intersection) / (union)  # (N, C)\n",
    "dice_loss = 1 - torch.mean(dice_coef)  # 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.9560)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiDiceLoss(nn.Module):\n",
    "    def __init__(self, ignore_index=255, epsilon=1e-5):\n",
    "        super(MultiDiceLoss, self).__init__()\n",
    "        self.ignore_index = ignore_index\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, prediction, target):\n",
    "        N,C,H,W = prediction.shape # prediction: [N, C, H, W]\n",
    "        prediction = F.softmax(prediction, dim=1) \n",
    "        dice = 0\n",
    "\n",
    "        for i in range(C):\n",
    "            target_mask = (target==i).float()\n",
    "            output_c = prediction[:,i,:,:]\n",
    "            intersection = (output_c*target_mask).sum()\n",
    "            union = target_mask.sum()+output_c.sum()\n",
    "            dice += (2*intersection+self.epsilon)/(union+self.epsilon)\n",
    "\n",
    "        return 1-dice/C\n",
    "\n",
    "criterion = MultiDiceLoss()\n",
    "loss_fn = criterion(outputs,labels)\n",
    "print(loss_fn)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
