{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23508032\n",
      "16130323\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import Cityscapes, wrap_dataset_for_transforms_v2\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.transforms.v2 import (\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    Resize,\n",
    "    ToImage,\n",
    "    ToDtype,\n",
    ")\n",
    "\n",
    "\n",
    "model = deeplabv3\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "for param in deeplabv3.backbone.parameters():\n",
    "    param.requires_grad = False  # Freeze the early layers\n",
    "\n",
    "for param in deeplabv3.backbone.layer4.parameters():  # Unfreeze only the last ResNet layer\n",
    "    param.requires_grad = True\n",
    "\n",
    "print(count_parameters(deeplabv3.backbone))\n",
    "print(count_parameters(deeplabv3.classifier))\n",
    "\n",
    "# Mapping class IDs to train IDs\n",
    "id_to_trainid = {cls.id: cls.train_id for cls in Cityscapes.classes}\n",
    "def convert_to_train_id(label_img: torch.Tensor) -> torch.Tensor:\n",
    "    return label_img.apply_(lambda x: id_to_trainid[x])\n",
    "\n",
    "# Mapping train IDs to color\n",
    "train_id_to_color = {cls.train_id: cls.color for cls in Cityscapes.classes if cls.train_id != 255}\n",
    "train_id_to_color[255] = (0, 0, 0)  # Assign black to ignored labels\n",
    "\n",
    "def convert_train_id_to_color(prediction: torch.Tensor) -> torch.Tensor:\n",
    "    batch, _, height, width = prediction.shape\n",
    "    color_image = torch.zeros((batch, 3, height, width), dtype=torch.uint8)\n",
    "\n",
    "    for train_id, color in train_id_to_color.items():\n",
    "        mask = prediction[:, 0] == train_id\n",
    "\n",
    "        for i in range(3):\n",
    "            color_image[:, i][mask] = color[i]\n",
    "\n",
    "    return color_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import deeplabv3 and change last layers to 19 classes instead of 21\n",
    "deeplabv3 = models.segmentation.deeplabv3_resnet50() #Use resnet50 because it is smaller than resnet101\n",
    "deeplabv3.classifier[4] = nn.Conv2d(256, 19, kernel_size=(1, 1))\n",
    "nn.init.xavier_normal_(deeplabv3.classifier[4].weight) #Initialize weights\n",
    "deeplab.backbone.layer4[0].conv2.dilation = (2, 2) #change  to stride 16\n",
    "deeplab.backbone.layer4[0].conv2.padding = (2, 2)\n",
    "deeplab.backbone.layer4[0].downsample[0].stride = (1, 1)  # Prevents downsampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CityscapesClass(name='unlabeled', id=0, train_id=255, category='void', category_id=0, has_instances=False, ignore_in_eval=True, color=(0, 0, 0)), CityscapesClass(name='ego vehicle', id=1, train_id=255, category='void', category_id=0, has_instances=False, ignore_in_eval=True, color=(0, 0, 0)), CityscapesClass(name='rectification border', id=2, train_id=255, category='void', category_id=0, has_instances=False, ignore_in_eval=True, color=(0, 0, 0)), CityscapesClass(name='out of roi', id=3, train_id=255, category='void', category_id=0, has_instances=False, ignore_in_eval=True, color=(0, 0, 0)), CityscapesClass(name='static', id=4, train_id=255, category='void', category_id=0, has_instances=False, ignore_in_eval=True, color=(0, 0, 0)), CityscapesClass(name='dynamic', id=5, train_id=255, category='void', category_id=0, has_instances=False, ignore_in_eval=True, color=(111, 74, 0)), CityscapesClass(name='ground', id=6, train_id=255, category='void', category_id=0, has_instances=False, ignore_in_eval=True, color=(81, 0, 81)), CityscapesClass(name='road', id=7, train_id=0, category='flat', category_id=1, has_instances=False, ignore_in_eval=False, color=(128, 64, 128)), CityscapesClass(name='sidewalk', id=8, train_id=1, category='flat', category_id=1, has_instances=False, ignore_in_eval=False, color=(244, 35, 232)), CityscapesClass(name='parking', id=9, train_id=255, category='flat', category_id=1, has_instances=False, ignore_in_eval=True, color=(250, 170, 160)), CityscapesClass(name='rail track', id=10, train_id=255, category='flat', category_id=1, has_instances=False, ignore_in_eval=True, color=(230, 150, 140)), CityscapesClass(name='building', id=11, train_id=2, category='construction', category_id=2, has_instances=False, ignore_in_eval=False, color=(70, 70, 70)), CityscapesClass(name='wall', id=12, train_id=3, category='construction', category_id=2, has_instances=False, ignore_in_eval=False, color=(102, 102, 156)), CityscapesClass(name='fence', id=13, train_id=4, category='construction', category_id=2, has_instances=False, ignore_in_eval=False, color=(190, 153, 153)), CityscapesClass(name='guard rail', id=14, train_id=255, category='construction', category_id=2, has_instances=False, ignore_in_eval=True, color=(180, 165, 180)), CityscapesClass(name='bridge', id=15, train_id=255, category='construction', category_id=2, has_instances=False, ignore_in_eval=True, color=(150, 100, 100)), CityscapesClass(name='tunnel', id=16, train_id=255, category='construction', category_id=2, has_instances=False, ignore_in_eval=True, color=(150, 120, 90)), CityscapesClass(name='pole', id=17, train_id=5, category='object', category_id=3, has_instances=False, ignore_in_eval=False, color=(153, 153, 153)), CityscapesClass(name='polegroup', id=18, train_id=255, category='object', category_id=3, has_instances=False, ignore_in_eval=True, color=(153, 153, 153)), CityscapesClass(name='traffic light', id=19, train_id=6, category='object', category_id=3, has_instances=False, ignore_in_eval=False, color=(250, 170, 30)), CityscapesClass(name='traffic sign', id=20, train_id=7, category='object', category_id=3, has_instances=False, ignore_in_eval=False, color=(220, 220, 0)), CityscapesClass(name='vegetation', id=21, train_id=8, category='nature', category_id=4, has_instances=False, ignore_in_eval=False, color=(107, 142, 35)), CityscapesClass(name='terrain', id=22, train_id=9, category='nature', category_id=4, has_instances=False, ignore_in_eval=False, color=(152, 251, 152)), CityscapesClass(name='sky', id=23, train_id=10, category='sky', category_id=5, has_instances=False, ignore_in_eval=False, color=(70, 130, 180)), CityscapesClass(name='person', id=24, train_id=11, category='human', category_id=6, has_instances=True, ignore_in_eval=False, color=(220, 20, 60)), CityscapesClass(name='rider', id=25, train_id=12, category='human', category_id=6, has_instances=True, ignore_in_eval=False, color=(255, 0, 0)), CityscapesClass(name='car', id=26, train_id=13, category='vehicle', category_id=7, has_instances=True, ignore_in_eval=False, color=(0, 0, 142)), CityscapesClass(name='truck', id=27, train_id=14, category='vehicle', category_id=7, has_instances=True, ignore_in_eval=False, color=(0, 0, 70)), CityscapesClass(name='bus', id=28, train_id=15, category='vehicle', category_id=7, has_instances=True, ignore_in_eval=False, color=(0, 60, 100)), CityscapesClass(name='caravan', id=29, train_id=255, category='vehicle', category_id=7, has_instances=True, ignore_in_eval=True, color=(0, 0, 90)), CityscapesClass(name='trailer', id=30, train_id=255, category='vehicle', category_id=7, has_instances=True, ignore_in_eval=True, color=(0, 0, 110)), CityscapesClass(name='train', id=31, train_id=16, category='vehicle', category_id=7, has_instances=True, ignore_in_eval=False, color=(0, 80, 100)), CityscapesClass(name='motorcycle', id=32, train_id=17, category='vehicle', category_id=7, has_instances=True, ignore_in_eval=False, color=(0, 0, 230)), CityscapesClass(name='bicycle', id=33, train_id=18, category='vehicle', category_id=7, has_instances=True, ignore_in_eval=False, color=(119, 11, 32)), CityscapesClass(name='license plate', id=-1, train_id=-1, category='vehicle', category_id=7, has_instances=False, ignore_in_eval=True, color=(0, 0, 142))]\n"
     ]
    }
   ],
   "source": [
    "print(Cityscapes.classes);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CityscapesClass(name='unlabeled', id=0, train_id=255, category='void', category_id=0, has_instances=False, ignore_in_eval=True, color=(0, 0, 0)),                  Ignored 255\n",
    "CityscapesClass(name='ego vehicle', id=1, train_id=255, category='void', category_id=0, has_instances=False, ignore_in_eval=True, color=(0, 0, 0)),                 Ignored 255\n",
    "CityscapesClass(name='rectification border', id=2, train_id=255, category='void', category_id=0, has_instances=False, ignore_in_eval=True, color=(0, 0, 0)),        Ignored 255\n",
    "CityscapesClass(name='out of roi', id=3, train_id=255, category='void', category_id=0, has_instances=False, ignore_in_eval=True, color=(0, 0, 0)),                  Ignored 255\n",
    "CityscapesClass(name='static', id=4, train_id=255, category='void', category_id=0, has_instances=False, ignore_in_eval=True, color=(0, 0, 0)),                      Ignored 255\n",
    "CityscapesClass(name='dynamic', id=5, train_id=255, category='void', category_id=0, has_instances=False, ignore_in_eval=True, color=(111, 74, 0)),                  Ignored 255\n",
    "CityscapesClass(name='ground', id=6, train_id=255, category='void', category_id=0, has_instances=False, ignore_in_eval=True, color=(81, 0, 81)),                    Ignored 255\n",
    "CityscapesClass(name='road', id=7, train_id=0, category='flat', category_id=1, has_instances=False, ignore_in_eval=False, color=(128, 64, 128)),                    0\n",
    "CityscapesClass(name='sidewalk', id=8, train_id=1, category='flat', category_id=1, has_instances=False, ignore_in_eval=False, color=(244, 35, 232)),                1\n",
    "CityscapesClass(name='parking', id=9, train_id=255, category='flat', category_id=1, has_instances=False, ignore_in_eval=True, color=(250, 170, 160)),               Ignored 255\n",
    "CityscapesClass(name='rail track', id=10, train_id=255, category='flat', category_id=1, has_instances=False, ignore_in_eval=True, color=(230, 150, 140)),           Ignored 255\n",
    "CityscapesClass(name='building', id=11, train_id=2, category='construction', category_id=2, has_instances=False, ignore_in_eval=False, color=(70, 70, 70)),         2\n",
    "CityscapesClass(name='wall', id=12, train_id=3, category='construction', category_id=2, has_instances=False, ignore_in_eval=False, color=(102, 102, 156)),          3\n",
    "CityscapesClass(name='fence', id=13, train_id=4, category='construction', category_id=2, has_instances=False, ignore_in_eval=False, color=(190, 153, 153)),         4\n",
    "CityscapesClass(name='guard rail', id=14, train_id=255, category='construction', category_id=2, has_instances=False, ignore_in_eval=True, color=(180, 165, 180)),   Ignored 255\n",
    "CityscapesClass(name='bridge', id=15, train_id=255, category='construction', category_id=2, has_instances=False, ignore_in_eval=True, color=(150, 100, 100)),       Ignored 255\n",
    "CityscapesClass(name='tunnel', id=16, train_id=255, category='construction', category_id=2, has_instances=False, ignore_in_eval=True, color=(150, 120, 90)),        5\n",
    "CityscapesClass(name='pole', id=17, train_id=5, category='object', category_id=3, has_instances=False, ignore_in_eval=False, color=(153, 153, 153)),                Ignored 255\n",
    "CityscapesClass(name='polegroup', id=18, train_id=255, category='object', category_id=3, has_instances=False, ignore_in_eval=True, color=(153, 153, 153)),          Ignored 255\n",
    "CityscapesClass(name='traffic light', id=19, train_id=6, category='object', category_id=3, has_instances=False, ignore_in_eval=False, color=(250, 170, 30)),        6\n",
    "CityscapesClass(name='traffic sign', id=20, train_id=7, category='object', category_id=3, has_instances=False, ignore_in_eval=False, color=(220, 220, 0)),          7\n",
    "CityscapesClass(name='vegetation', id=21, train_id=8, category='nature', category_id=4, has_instances=False, ignore_in_eval=False, color=(107, 142, 35)),           8\n",
    "CityscapesClass(name='terrain', id=22, train_id=9, category='nature', category_id=4, has_instances=False, ignore_in_eval=False, color=(152, 251, 152)),             9\n",
    "CityscapesClass(name='sky', id=23, train_id=10, category='sky', category_id=5, has_instances=False, ignore_in_eval=False, color=(70, 130, 180)),                    10\n",
    "CityscapesClass(name='person', id=24, train_id=11, category='human', category_id=6, has_instances=True, ignore_in_eval=False, color=(220, 20, 60)),                 11\n",
    "CityscapesClass(name='rider', id=25, train_id=12, category='human', category_id=6, has_instances=True, ignore_in_eval=False, color=(255, 0, 0)),                    12\n",
    "CityscapesClass(name='car', id=26, train_id=13, category='vehicle', category_id=7, has_instances=True, ignore_in_eval=False, color=(0, 0, 142)),                    13\n",
    "CityscapesClass(name='truck', id=27, train_id=14, category='vehicle', category_id=7, has_instances=True, ignore_in_eval=False, color=(0, 0, 70)),                   14\n",
    "CityscapesClass(name='bus', id=28, train_id=15, category='vehicle', category_id=7, has_instances=True, ignore_in_eval=False, color=(0, 60, 100)),                   15\n",
    "CityscapesClass(name='caravan', id=29, train_id=255, category='vehicle', category_id=7, has_instances=True, ignore_in_eval=True, color=(0, 0, 90)),                 Ignored 255\n",
    "CityscapesClass(name='trailer', id=30, train_id=255, category='vehicle', category_id=7, has_instances=True, ignore_in_eval=True, color=(0, 0, 110)),                Ignored 255\n",
    "CityscapesClass(name='train', id=31, train_id=16, category='vehicle', category_id=7, has_instances=True, ignore_in_eval=False, color=(0, 80, 100)),                 16\n",
    "CityscapesClass(name='motorcycle', id=32, train_id=17, category='vehicle', category_id=7, has_instances=True, ignore_in_eval=False, color=(0, 0, 230)),             17\n",
    "CityscapesClass(name='bicycle', id=33, train_id=18, category='vehicle', category_id=7, has_instances=True, ignore_in_eval=False, color=(119, 11, 32)),              18\n",
    "CityscapesClass(name='license plate', id=-1, train_id=-1, category='vehicle', category_id=7, has_instances=False, ignore_in_eval=True, color=(0, 0, 142))]          -1\n",
    "\n",
    "Classes: Road, sidewalk, building, wall, fence, tunnel, traffic light, traffic sign, vegetation terrain, sky, person, rider, car, truck, train, motorcycle, bicycle, license plate\n",
    "Excluded: Unlabeled, ego vehicle, rectification border, out or roi, static, dynamic, ground, parking, rail track, guard rail, bridge, pole, polegroup, caravan, trailer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducability\n",
    "# If you add other sources of randomness (NumPy, Random), \n",
    "# make sure to set their seeds as well\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7015, 0.7890, 0.7338, 0.9640, 0.8868, 0.9051, 0.8909, 0.8425, 0.9585,\n",
       "        0.7420, 0.8541, 0.7460, 0.8897, 0.7951, 0.8894, 0.8148, 0.9653, 0.7591,\n",
       "        0.7585, 0.7585, 0.9754, 0.8933, 0.9845, 0.7253, 0.7012, 0.7316, 0.7475,\n",
       "        0.9037, 0.8969, 0.9655, 0.7701, 0.7462, 0.9219, 0.7864, 0.9330, 0.8106,\n",
       "        0.9279, 0.7323, 0.7718, 0.8892, 0.8757, 0.8080, 0.9314, 0.9436, 0.7398,\n",
       "        0.7676, 0.9778, 0.7961, 0.7936, 0.7047, 0.7620, 0.8812, 0.8259, 0.7397,\n",
       "        0.8484, 0.7460, 0.7220, 0.7652, 0.7181, 0.7527, 0.9899, 0.8724, 0.8897,\n",
       "        0.7098, 0.7498, 0.7967, 0.8677, 0.7174, 0.7825, 0.7582, 0.8454, 0.7910,\n",
       "        0.8350, 0.7467, 0.7455, 0.7604, 0.7954, 0.7306, 0.9666, 0.8162, 0.9698,\n",
       "        0.8902, 0.7222, 0.9453, 0.8051, 0.7894, 0.7246, 0.7008, 0.8865, 0.8133,\n",
       "        0.9015, 0.7260, 0.9527, 0.7386, 0.8200, 0.8753, 0.9199, 0.9621, 0.9771,\n",
       "        0.7300])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomgetal = torch.rand(100)*0.29+0.7\n",
    "randomgetal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Dataset not found or incomplete. Please make sure all required folders for the specified \"split\" and \"mode\" are inside the \"root\" directory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 11\u001b[0m\n\u001b[0;32m      3\u001b[0m transform \u001b[38;5;241m=\u001b[39m Compose([\n\u001b[0;32m      4\u001b[0m     ToImage(),\n\u001b[0;32m      5\u001b[0m     Resize((\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m)),\n\u001b[0;32m      6\u001b[0m     ToDtype(torch\u001b[38;5;241m.\u001b[39mfloat32, scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[0;32m      7\u001b[0m     Normalize((\u001b[38;5;241m0.485\u001b[39m, \u001b[38;5;241m0.456\u001b[39m, \u001b[38;5;241m0.406\u001b[39m), (\u001b[38;5;241m0.229\u001b[39m, \u001b[38;5;241m0.224\u001b[39m, \u001b[38;5;241m0.225\u001b[39m)),\n\u001b[0;32m      8\u001b[0m ])\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Load the dataset and make a split for training and validation\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mCityscapes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/cityscapes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfine\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msemantic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m valid_dataset \u001b[38;5;241m=\u001b[39m Cityscapes(\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/cityscapes\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m     20\u001b[0m     split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m     transforms\u001b[38;5;241m=\u001b[39mtransform\n\u001b[0;32m     24\u001b[0m )\n\u001b[0;32m     26\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m wrap_dataset_for_transforms_v2(train_dataset)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchvision\\datasets\\cityscapes.py:156\u001b[0m, in \u001b[0;36mCityscapes.__init__\u001b[1;34m(self, root, split, mode, target_type, transform, target_transform, transforms)\u001b[0m\n\u001b[0;32m    154\u001b[0m         extract_archive(from_path\u001b[38;5;241m=\u001b[39mtarget_dir_zip, to_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot)\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 156\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    157\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found or incomplete. Please make sure all required folders for the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    158\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m specified \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m are inside the \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m directory\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    159\u001b[0m         )\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m city \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages_dir):\n\u001b[0;32m    162\u001b[0m     img_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages_dir, city)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Dataset not found or incomplete. Please make sure all required folders for the specified \"split\" and \"mode\" are inside the \"root\" directory"
     ]
    }
   ],
   "source": [
    "# Define the transforms to apply to the data\n",
    "\n",
    "transform = Compose([\n",
    "    ToImage(),\n",
    "    Resize((256, 256)),\n",
    "    ToDtype(torch.float32, scale=True),\n",
    "    Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "# Load the dataset and make a split for training and validation\n",
    "train_dataset = Cityscapes(\n",
    "    \"data/cityscapes\", \n",
    "    split=\"train\", \n",
    "    mode=\"fine\", \n",
    "    target_type=\"semantic\", \n",
    "    transforms=transform\n",
    ")\n",
    "valid_dataset = Cityscapes(\n",
    "    \"data/cityscapes\", \n",
    "    split=\"val\", \n",
    "    mode=\"fine\", \n",
    "    target_type=\"semantic\", \n",
    "    transforms=transform\n",
    ")\n",
    "\n",
    "train_dataset = wrap_dataset_for_transforms_v2(train_dataset)\n",
    "valid_dataset = wrap_dataset_for_transforms_v2(valid_dataset)\n",
    "\n",
    " train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=64, \n",
    "    shuffle=True,\n",
    "    num_workers=9\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size=64, \n",
    "    shuffle=False,\n",
    "    num_workers=9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "# Define the model\n",
    "model = deeplabv3.to(device)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=255)  # Ignore the void class\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = AdamW(model.classifier.parameters(), lr=0.001)\n",
    "\n",
    "scheduler = lr_scheduler.MultiplicativeLR(optimizer, 0.7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0001/0010\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'wandb' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     22\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m---> 24\u001b[0m     \u001b[43mwandb\u001b[49m\u001b[38;5;241m.\u001b[39mlog({\n\u001b[1;32m     25\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: loss\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m     26\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m: optimizer\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m: epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     28\u001b[0m     }, step\u001b[38;5;241m=\u001b[39mepoch \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_dataloader) \u001b[38;5;241m+\u001b[39m i)\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Validation\u001b[39;00m\n\u001b[1;32m     31\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'wandb' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Training loop\n",
    "best_valid_loss = float('inf')\n",
    "current_best_model_path = None\n",
    "for epoch in range(10):\n",
    "    print(f\"Epoch {epoch+1:04}/{10:04}\")\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(train_dataloader):\n",
    "\n",
    "        labels = convert_to_train_id(labels)  # Convert class IDs to train IDs\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        labels = labels.long().squeeze(1)  # Remove channel dimension\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)['out']\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        losses = []\n",
    "        for i, (images, labels) in enumerate(valid_dataloader):\n",
    "\n",
    "            labels = convert_to_train_id(labels)  # Convert class IDs to train IDs\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            labels = labels.long().squeeze(1)  # Remove channel dimension\n",
    "\n",
    "            outputs = model(images)['out']\n",
    "            loss = criterion(outputs, labels)\n",
    "            losses.append(loss.item())\n",
    "        \n",
    "            if i == 0:\n",
    "                predictions = outputs.softmax(1).argmax(1)\n",
    "\n",
    "                predictions = predictions.unsqueeze(1)\n",
    "                labels = labels.unsqueeze(1)\n",
    "\n",
    "                predictions = convert_train_id_to_color(predictions)\n",
    "                labels = convert_train_id_to_color(labels)\n",
    "\n",
    "                predictions_img = make_grid(predictions.cpu(), nrow=8)\n",
    "                labels_img = make_grid(labels.cpu(), nrow=8)\n",
    "\n",
    "                predictions_img = predictions_img.permute(1, 2, 0).numpy()\n",
    "                labels_img = labels_img.permute(1, 2, 0).numpy()\n",
    "\n",
    "        \n",
    "        valid_loss = sum(losses) / len(losses)\n",
    "       \n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            if current_best_model_path:\n",
    "                os.remove(current_best_model_path)\n",
    "            current_best_model_path = os.path.join(\n",
    "                output_dir, \n",
    "                f\"best_model-epoch={epoch:04}-val_loss={valid_loss:04}.pth\"\n",
    "            )\n",
    "            torch.save(model.state_dict(), current_best_model_path)\n",
    "    \n",
    "print(\"Training complete!\")\n",
    "\n",
    "# Save the model\n",
    "torch.save(\n",
    "    model.state_dict(),\n",
    "    os.path.join(\n",
    "        output_dir,\n",
    "        f\"final_model-epoch={epoch:04}-val_loss={valid_loss:04}.pth\"\n",
    "    )\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
