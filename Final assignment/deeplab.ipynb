{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import Cityscapes, wrap_dataset_for_transforms_v2\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.transforms.v2 import (\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    Resize,\n",
    "    ToImage,\n",
    "    ToDtype,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomVerticalFlip,\n",
    ")\n",
    "\n",
    "# Mapping class IDs to train IDs\n",
    "id_to_trainid = {cls.id: cls.train_id for cls in Cityscapes.classes}\n",
    "def convert_to_train_id(label_img: torch.Tensor) -> torch.Tensor:\n",
    "    return label_img.apply_(lambda x: id_to_trainid[x])\n",
    "\n",
    "# Mapping train IDs to color\n",
    "train_id_to_color = {cls.train_id: cls.color for cls in Cityscapes.classes if cls.train_id != 255}\n",
    "train_id_to_color[255] = (0, 0, 0)  # Assign black to ignored labels\n",
    "\n",
    "def convert_train_id_to_color(prediction: torch.Tensor) -> torch.Tensor:\n",
    "    batch, _, height, width = prediction.shape\n",
    "    color_image = torch.zeros((batch, 3, height, width), dtype=torch.uint8)\n",
    "\n",
    "    for train_id, color in train_id_to_color.items():\n",
    "        mask = prediction[:, 0] == train_id\n",
    "\n",
    "        for i in range(3):\n",
    "            color_image[:, i][mask] = color[i]\n",
    "\n",
    "    return color_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23508032\n",
      "16130323\n"
     ]
    }
   ],
   "source": [
    "#Import deeplabv3 and change last layers to 19 classes instead of 21\n",
    "deeplabv3 = models.segmentation.deeplabv3_resnet50() #Use resnet50 because it is smaller than resnet101\n",
    "deeplabv3.classifier[4] = nn.Conv2d(256, 19, kernel_size=(1, 1))\n",
    "nn.init.xavier_normal_(deeplabv3.classifier[4].weight) #Initialize weights\n",
    "deeplabv3.backbone.layer4[0].conv2.dilation = (2, 2) #change  to stride 16\n",
    "deeplabv3.backbone.layer4[0].conv2.padding = (2, 2)\n",
    "deeplabv3.backbone.layer4[0].downsample[0].stride = (1, 1)  # Prevents downsampling\n",
    "\n",
    "model = deeplabv3\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "for param in deeplabv3.backbone.parameters():\n",
    "    param.requires_grad = False  # Freeze the early layers\n",
    "\n",
    "for param in deeplabv3.backbone.layer4.parameters():  # Unfreeze only the last ResNet layer\n",
    "    param.requires_grad = True\n",
    "\n",
    "print(count_parameters(deeplabv3.backbone))\n",
    "print(count_parameters(deeplabv3.classifier))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CityscapesClass(name='unlabeled', id=0, train_id=255, category='void', category_id=0, has_instances=False, ignore_in_eval=True, color=(0, 0, 0)), CityscapesClass(name='ego vehicle', id=1, train_id=255, category='void', category_id=0, has_instances=False, ignore_in_eval=True, color=(0, 0, 0)), CityscapesClass(name='rectification border', id=2, train_id=255, category='void', category_id=0, has_instances=False, ignore_in_eval=True, color=(0, 0, 0)), CityscapesClass(name='out of roi', id=3, train_id=255, category='void', category_id=0, has_instances=False, ignore_in_eval=True, color=(0, 0, 0)), CityscapesClass(name='static', id=4, train_id=255, category='void', category_id=0, has_instances=False, ignore_in_eval=True, color=(0, 0, 0)), CityscapesClass(name='dynamic', id=5, train_id=255, category='void', category_id=0, has_instances=False, ignore_in_eval=True, color=(111, 74, 0)), CityscapesClass(name='ground', id=6, train_id=255, category='void', category_id=0, has_instances=False, ignore_in_eval=True, color=(81, 0, 81)), CityscapesClass(name='road', id=7, train_id=0, category='flat', category_id=1, has_instances=False, ignore_in_eval=False, color=(128, 64, 128)), CityscapesClass(name='sidewalk', id=8, train_id=1, category='flat', category_id=1, has_instances=False, ignore_in_eval=False, color=(244, 35, 232)), CityscapesClass(name='parking', id=9, train_id=255, category='flat', category_id=1, has_instances=False, ignore_in_eval=True, color=(250, 170, 160)), CityscapesClass(name='rail track', id=10, train_id=255, category='flat', category_id=1, has_instances=False, ignore_in_eval=True, color=(230, 150, 140)), CityscapesClass(name='building', id=11, train_id=2, category='construction', category_id=2, has_instances=False, ignore_in_eval=False, color=(70, 70, 70)), CityscapesClass(name='wall', id=12, train_id=3, category='construction', category_id=2, has_instances=False, ignore_in_eval=False, color=(102, 102, 156)), CityscapesClass(name='fence', id=13, train_id=4, category='construction', category_id=2, has_instances=False, ignore_in_eval=False, color=(190, 153, 153)), CityscapesClass(name='guard rail', id=14, train_id=255, category='construction', category_id=2, has_instances=False, ignore_in_eval=True, color=(180, 165, 180)), CityscapesClass(name='bridge', id=15, train_id=255, category='construction', category_id=2, has_instances=False, ignore_in_eval=True, color=(150, 100, 100)), CityscapesClass(name='tunnel', id=16, train_id=255, category='construction', category_id=2, has_instances=False, ignore_in_eval=True, color=(150, 120, 90)), CityscapesClass(name='pole', id=17, train_id=5, category='object', category_id=3, has_instances=False, ignore_in_eval=False, color=(153, 153, 153)), CityscapesClass(name='polegroup', id=18, train_id=255, category='object', category_id=3, has_instances=False, ignore_in_eval=True, color=(153, 153, 153)), CityscapesClass(name='traffic light', id=19, train_id=6, category='object', category_id=3, has_instances=False, ignore_in_eval=False, color=(250, 170, 30)), CityscapesClass(name='traffic sign', id=20, train_id=7, category='object', category_id=3, has_instances=False, ignore_in_eval=False, color=(220, 220, 0)), CityscapesClass(name='vegetation', id=21, train_id=8, category='nature', category_id=4, has_instances=False, ignore_in_eval=False, color=(107, 142, 35)), CityscapesClass(name='terrain', id=22, train_id=9, category='nature', category_id=4, has_instances=False, ignore_in_eval=False, color=(152, 251, 152)), CityscapesClass(name='sky', id=23, train_id=10, category='sky', category_id=5, has_instances=False, ignore_in_eval=False, color=(70, 130, 180)), CityscapesClass(name='person', id=24, train_id=11, category='human', category_id=6, has_instances=True, ignore_in_eval=False, color=(220, 20, 60)), CityscapesClass(name='rider', id=25, train_id=12, category='human', category_id=6, has_instances=True, ignore_in_eval=False, color=(255, 0, 0)), CityscapesClass(name='car', id=26, train_id=13, category='vehicle', category_id=7, has_instances=True, ignore_in_eval=False, color=(0, 0, 142)), CityscapesClass(name='truck', id=27, train_id=14, category='vehicle', category_id=7, has_instances=True, ignore_in_eval=False, color=(0, 0, 70)), CityscapesClass(name='bus', id=28, train_id=15, category='vehicle', category_id=7, has_instances=True, ignore_in_eval=False, color=(0, 60, 100)), CityscapesClass(name='caravan', id=29, train_id=255, category='vehicle', category_id=7, has_instances=True, ignore_in_eval=True, color=(0, 0, 90)), CityscapesClass(name='trailer', id=30, train_id=255, category='vehicle', category_id=7, has_instances=True, ignore_in_eval=True, color=(0, 0, 110)), CityscapesClass(name='train', id=31, train_id=16, category='vehicle', category_id=7, has_instances=True, ignore_in_eval=False, color=(0, 80, 100)), CityscapesClass(name='motorcycle', id=32, train_id=17, category='vehicle', category_id=7, has_instances=True, ignore_in_eval=False, color=(0, 0, 230)), CityscapesClass(name='bicycle', id=33, train_id=18, category='vehicle', category_id=7, has_instances=True, ignore_in_eval=False, color=(119, 11, 32)), CityscapesClass(name='license plate', id=-1, train_id=-1, category='vehicle', category_id=7, has_instances=False, ignore_in_eval=True, color=(0, 0, 142))]\n"
     ]
    }
   ],
   "source": [
    "print(Cityscapes.classes);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[CityscapesClass(name='unlabeled', id=0, train_id=255, category='void', category_id=0, has_instances=False, ignore_in_eval=True, color=(0, 0, 0)),                  Ignored 255\n",
    "CityscapesClass(name='ego vehicle', id=1, train_id=255, category='void', category_id=0, has_instances=False, ignore_in_eval=True, color=(0, 0, 0)),                 Ignored 255\n",
    "CityscapesClass(name='rectification border', id=2, train_id=255, category='void', category_id=0, has_instances=False, ignore_in_eval=True, color=(0, 0, 0)),        Ignored 255\n",
    "CityscapesClass(name='out of roi', id=3, train_id=255, category='void', category_id=0, has_instances=False, ignore_in_eval=True, color=(0, 0, 0)),                  Ignored 255\n",
    "CityscapesClass(name='static', id=4, train_id=255, category='void', category_id=0, has_instances=False, ignore_in_eval=True, color=(0, 0, 0)),                      Ignored 255\n",
    "CityscapesClass(name='dynamic', id=5, train_id=255, category='void', category_id=0, has_instances=False, ignore_in_eval=True, color=(111, 74, 0)),                  Ignored 255\n",
    "CityscapesClass(name='ground', id=6, train_id=255, category='void', category_id=0, has_instances=False, ignore_in_eval=True, color=(81, 0, 81)),                    Ignored 255\n",
    "CityscapesClass(name='road', id=7, train_id=0, category='flat', category_id=1, has_instances=False, ignore_in_eval=False, color=(128, 64, 128)),                    0\n",
    "CityscapesClass(name='sidewalk', id=8, train_id=1, category='flat', category_id=1, has_instances=False, ignore_in_eval=False, color=(244, 35, 232)),                1\n",
    "CityscapesClass(name='parking', id=9, train_id=255, category='flat', category_id=1, has_instances=False, ignore_in_eval=True, color=(250, 170, 160)),               Ignored 255\n",
    "CityscapesClass(name='rail track', id=10, train_id=255, category='flat', category_id=1, has_instances=False, ignore_in_eval=True, color=(230, 150, 140)),           Ignored 255\n",
    "CityscapesClass(name='building', id=11, train_id=2, category='construction', category_id=2, has_instances=False, ignore_in_eval=False, color=(70, 70, 70)),         2\n",
    "CityscapesClass(name='wall', id=12, train_id=3, category='construction', category_id=2, has_instances=False, ignore_in_eval=False, color=(102, 102, 156)),          3\n",
    "CityscapesClass(name='fence', id=13, train_id=4, category='construction', category_id=2, has_instances=False, ignore_in_eval=False, color=(190, 153, 153)),         4\n",
    "CityscapesClass(name='guard rail', id=14, train_id=255, category='construction', category_id=2, has_instances=False, ignore_in_eval=True, color=(180, 165, 180)),   Ignored 255\n",
    "CityscapesClass(name='bridge', id=15, train_id=255, category='construction', category_id=2, has_instances=False, ignore_in_eval=True, color=(150, 100, 100)),       Ignored 255\n",
    "CityscapesClass(name='tunnel', id=16, train_id=255, category='construction', category_id=2, has_instances=False, ignore_in_eval=True, color=(150, 120, 90)),        5\n",
    "CityscapesClass(name='pole', id=17, train_id=5, category='object', category_id=3, has_instances=False, ignore_in_eval=False, color=(153, 153, 153)),                Ignored 255\n",
    "CityscapesClass(name='polegroup', id=18, train_id=255, category='object', category_id=3, has_instances=False, ignore_in_eval=True, color=(153, 153, 153)),          Ignored 255\n",
    "CityscapesClass(name='traffic light', id=19, train_id=6, category='object', category_id=3, has_instances=False, ignore_in_eval=False, color=(250, 170, 30)),        6\n",
    "CityscapesClass(name='traffic sign', id=20, train_id=7, category='object', category_id=3, has_instances=False, ignore_in_eval=False, color=(220, 220, 0)),          7\n",
    "CityscapesClass(name='vegetation', id=21, train_id=8, category='nature', category_id=4, has_instances=False, ignore_in_eval=False, color=(107, 142, 35)),           8\n",
    "CityscapesClass(name='terrain', id=22, train_id=9, category='nature', category_id=4, has_instances=False, ignore_in_eval=False, color=(152, 251, 152)),             9\n",
    "CityscapesClass(name='sky', id=23, train_id=10, category='sky', category_id=5, has_instances=False, ignore_in_eval=False, color=(70, 130, 180)),                    10\n",
    "CityscapesClass(name='person', id=24, train_id=11, category='human', category_id=6, has_instances=True, ignore_in_eval=False, color=(220, 20, 60)),                 11\n",
    "CityscapesClass(name='rider', id=25, train_id=12, category='human', category_id=6, has_instances=True, ignore_in_eval=False, color=(255, 0, 0)),                    12\n",
    "CityscapesClass(name='car', id=26, train_id=13, category='vehicle', category_id=7, has_instances=True, ignore_in_eval=False, color=(0, 0, 142)),                    13\n",
    "CityscapesClass(name='truck', id=27, train_id=14, category='vehicle', category_id=7, has_instances=True, ignore_in_eval=False, color=(0, 0, 70)),                   14\n",
    "CityscapesClass(name='bus', id=28, train_id=15, category='vehicle', category_id=7, has_instances=True, ignore_in_eval=False, color=(0, 60, 100)),                   15\n",
    "CityscapesClass(name='caravan', id=29, train_id=255, category='vehicle', category_id=7, has_instances=True, ignore_in_eval=True, color=(0, 0, 90)),                 Ignored 255\n",
    "CityscapesClass(name='trailer', id=30, train_id=255, category='vehicle', category_id=7, has_instances=True, ignore_in_eval=True, color=(0, 0, 110)),                Ignored 255\n",
    "CityscapesClass(name='train', id=31, train_id=16, category='vehicle', category_id=7, has_instances=True, ignore_in_eval=False, color=(0, 80, 100)),                 16\n",
    "CityscapesClass(name='motorcycle', id=32, train_id=17, category='vehicle', category_id=7, has_instances=True, ignore_in_eval=False, color=(0, 0, 230)),             17\n",
    "CityscapesClass(name='bicycle', id=33, train_id=18, category='vehicle', category_id=7, has_instances=True, ignore_in_eval=False, color=(119, 11, 32)),              18\n",
    "CityscapesClass(name='license plate', id=-1, train_id=-1, category='vehicle', category_id=7, has_instances=False, ignore_in_eval=True, color=(0, 0, 142))]          -1\n",
    "\n",
    "Classes: Road, sidewalk, building, wall, fence, tunnel, traffic light, traffic sign, vegetation terrain, sky, person, rider, car, truck, train, motorcycle, bicycle, license plate\n",
    "Excluded: Unlabeled, ego vehicle, rectification border, out or roi, static, dynamic, ground, parking, rail track, guard rail, bridge, pole, polegroup, caravan, trailer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducability\n",
    "# If you add other sources of randomness (NumPy, Random), \n",
    "# make sure to set their seeds as well\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Dataset not found or incomplete. Please make sure all required folders for the specified \"split\" and \"mode\" are inside the \"root\" directory",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 46\u001b[0m\n\u001b[0;32m     35\u001b[0m transform \u001b[38;5;241m=\u001b[39m Compose([\n\u001b[0;32m     36\u001b[0m     ToImage(),\n\u001b[0;32m     37\u001b[0m     Resize((\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m)),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m     RandomVerticalFlip(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m),\n\u001b[0;32m     43\u001b[0m ])\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Load the dataset and make a split for training and validation\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mCityscapes\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdata/cityscapes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfine\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msemantic\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m valid_dataset \u001b[38;5;241m=\u001b[39m Cityscapes(\n\u001b[0;32m     54\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata/cityscapes\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[0;32m     55\u001b[0m     split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     58\u001b[0m     transforms\u001b[38;5;241m=\u001b[39mtransform\n\u001b[0;32m     59\u001b[0m )\n\u001b[0;32m     61\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m wrap_dataset_for_transforms_v2(train_dataset)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\torchvision\\datasets\\cityscapes.py:156\u001b[0m, in \u001b[0;36mCityscapes.__init__\u001b[1;34m(self, root, split, mode, target_type, transform, target_transform, transforms)\u001b[0m\n\u001b[0;32m    154\u001b[0m         extract_archive(from_path\u001b[38;5;241m=\u001b[39mtarget_dir_zip, to_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot)\n\u001b[0;32m    155\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 156\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    157\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset not found or incomplete. Please make sure all required folders for the\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    158\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m specified \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msplit\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m and \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m are inside the \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m directory\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    159\u001b[0m         )\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m city \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages_dir):\n\u001b[0;32m    162\u001b[0m     img_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimages_dir, city)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Dataset not found or incomplete. Please make sure all required folders for the specified \"split\" and \"mode\" are inside the \"root\" directory"
     ]
    }
   ],
   "source": [
    "# Define the transforms to apply to the data\n",
    "class PaintingByNumbersTransform:\n",
    "      def __init__(self, id_to_color=None):\n",
    "          self.id_to_color = id_to_color  # Dictionary mapping class IDs to colors\n",
    "  \n",
    "      def random_recolor(self, label_img):\n",
    "          \"\"\"Assigns random colors to segmentation labels.\"\"\"\n",
    "          h, w = label_img.shape[1:]\n",
    "          recolored = torch.zeros((3, h, w), dtype=torch.uint8)  # Create an empty RGB image\n",
    "                  \n",
    "          unique_labels = label_img.unique()\n",
    "          color_map = {label.item(): torch.randint(0, 256, (3,), dtype=torch.uint8) for label in unique_labels}\n",
    "          \n",
    "          for label, color in color_map.items():\n",
    "              mask = label_img == label  # Shape: (h, w)\n",
    "              recolored[:, mask] = color  # Broadcasting works correctly\n",
    "             \n",
    "          return recolored\n",
    "  \n",
    "      def __call__(self, img, target):\n",
    "          if torch.rand(1).item() > 0.5:\n",
    "              # Load the actual ground truth color image\n",
    "              gt_color = self.random_recolor(target)\n",
    "  \n",
    "              # Blend image and color segmentation map\n",
    "              alpha = torch.rand(1).item() * 0.29 + 0.7  # Random alpha between 0.7 and 0.99\n",
    "              blended_img = alpha * img + (1 - alpha) * gt_color.float() / 255.0\n",
    "              print(blended_img.shape)\n",
    "              return blended_img, target\n",
    "          \n",
    "          return img, target  # If not applying transformation, return original\n",
    "\n",
    "\n",
    "transform = Compose([\n",
    "    ToImage(),\n",
    "    Resize((256, 256)),\n",
    "    ToDtype(torch.float32, scale=True),\n",
    "    Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    #PaintingByNumbersTransform(),\n",
    "    RandomHorizontalFlip(p=0.5),\n",
    "    RandomVerticalFlip(p=0.5),\n",
    "])\n",
    "\n",
    "# Load the dataset and make a split for training and validation\n",
    "train_dataset = Cityscapes(\n",
    "    \"data/cityscapes\", \n",
    "    split=\"train\", \n",
    "    mode=\"fine\", \n",
    "    target_type=\"semantic\", \n",
    "    transforms=transform\n",
    ")\n",
    "valid_dataset = Cityscapes(\n",
    "    \"data/cityscapes\", \n",
    "    split=\"val\", \n",
    "    mode=\"fine\", \n",
    "    target_type=\"semantic\", \n",
    "    transforms=transform\n",
    ")\n",
    "\n",
    "train_dataset = wrap_dataset_for_transforms_v2(train_dataset)\n",
    "valid_dataset = wrap_dataset_for_transforms_v2(valid_dataset)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=64, \n",
    "    shuffle=True,\n",
    "    num_workers=9\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size=64, \n",
    "    shuffle=False,\n",
    "    num_workers=9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recoloring(target):\n",
    "    recolored = torch.zeros((3, 3, 3), dtype=torch.uint8)  # Create an empty RGB image\n",
    "    unique_labels = label_img.unique()\n",
    "    color_map = {label.item(): torch.randint(0, 256, (3,), dtype=torch.uint8) for label in unique_labels}\n",
    "\n",
    "    for label, color in color_map.items():\n",
    "        mask = label_img == label  # Shape: (h, w)\n",
    "        recolored[mask] = color  # Assign the color to the masked pixels\n",
    "    return recolored\n",
    "\n",
    "label_img = torch.tensor([[1,1,1],[2,3,2],[4,4,4]])\n",
    "color_img = recoloring(label_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "# Define the model\n",
    "model = deeplabv3.to(device)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=255)  # Ignore the void class\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = AdamW(model.classifier.parameters(), lr=0.001)\n",
    "scheduler = lr_scheduler.MultiplicativeLR(optimizer,lambda epoch: 0.7)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0001/0010\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[1;32m     10\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, (images, labels) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m     13\u001b[0m     labels \u001b[38;5;241m=\u001b[39m convert_to_train_id(labels)  \u001b[38;5;66;03m# Convert class IDs to train IDs\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     images, labels \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1458\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1457\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1458\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1459\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1460\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1461\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1420\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1416\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1417\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1418\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1419\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1420\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1421\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1422\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1251\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1238\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1239\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1240\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1249\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1250\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1251\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1252\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1253\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1254\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1255\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1256\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m/sw/arch/RHEL8/EB_production/2023/software/Python/3.11.3-GCCcore-12.3.0/lib/python3.11/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block:\n\u001b[1;32m    112\u001b[0m     timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_poll():\n",
      "File \u001b[0;32m/sw/arch/RHEL8/EB_production/2023/software/Python/3.11.3-GCCcore-12.3.0/lib/python3.11/multiprocessing/connection.py:256\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_closed()\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 256\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/sw/arch/RHEL8/EB_production/2023/software/Python/3.11.3-GCCcore-12.3.0/lib/python3.11/multiprocessing/connection.py:423\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 423\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    424\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m/sw/arch/RHEL8/EB_production/2023/software/Python/3.11.3-GCCcore-12.3.0/lib/python3.11/multiprocessing/connection.py:930\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    927\u001b[0m     deadline \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic() \u001b[38;5;241m+\u001b[39m timeout\n\u001b[1;32m    929\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 930\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    931\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n\u001b[1;32m    932\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [key\u001b[38;5;241m.\u001b[39mfileobj \u001b[38;5;28;01mfor\u001b[39;00m (key, events) \u001b[38;5;129;01min\u001b[39;00m ready]\n",
      "File \u001b[0;32m/sw/arch/RHEL8/EB_production/2023/software/Python/3.11.3-GCCcore-12.3.0/lib/python3.11/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    413\u001b[0m ready \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_selector\u001b[38;5;241m.\u001b[39mpoll(timeout)\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n\u001b[1;32m    417\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ready\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Training loop\n",
    "best_valid_loss = float('inf')\n",
    "current_best_model_path = None\n",
    "for epoch in range(10):\n",
    "    print(f\"Epoch {epoch+1:04}/{10:04}\")\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(train_dataloader):\n",
    "\n",
    "        labels = convert_to_train_id(labels)  # Convert class IDs to train IDs\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        labels = labels.long().squeeze(1)  # Remove channel dimension\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)['out']\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        losses = []\n",
    "        for i, (images, labels) in enumerate(valid_dataloader):\n",
    "            labels = convert_to_train_id(labels)  # Convert class IDs to train IDs\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            labels = labels.long().squeeze(1)  # Remove channel dimension\n",
    "\n",
    "            outputs = model(images)['out']\n",
    "            loss = criterion(outputs, labels)\n",
    "            losses.append(loss.item())\n",
    "        \n",
    "            if i == 0:\n",
    "                predictions = outputs.softmax(1).argmax(1)\n",
    "\n",
    "                predictions = predictions.unsqueeze(1)\n",
    "                labels = labels.unsqueeze(1)\n",
    "\n",
    "                predictions = convert_train_id_to_color(predictions)\n",
    "                labels = convert_train_id_to_color(labels)\n",
    "\n",
    "                predictions_img = make_grid(predictions.cpu(), nrow=8)\n",
    "                labels_img = make_grid(labels.cpu(), nrow=8)\n",
    "\n",
    "                predictions_img = predictions_img.permute(1, 2, 0).numpy()\n",
    "                labels_img = labels_img.permute(1, 2, 0).numpy()\n",
    "\n",
    "        \n",
    "        valid_loss = sum(losses) / len(losses)\n",
    "       \n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            if current_best_model_path:\n",
    "                os.remove(current_best_model_path)\n",
    "            current_best_model_path = os.path.join(\n",
    "                output_dir, \n",
    "                f\"best_model-epoch={epoch:04}-val_loss={valid_loss:04}.pth\"\n",
    "            )\n",
    "            torch.save(model.state_dict(), current_best_model_path)\n",
    "    \n",
    "print(\"Training complete!\")\n",
    "\n",
    "# Save the model\n",
    "torch.save(\n",
    "    model.state_dict(),\n",
    "    os.path.join(\n",
    "        output_dir,\n",
    "        f\"final_model-epoch={epoch:04}-val_loss={valid_loss:04}.pth\"\n",
    "    )\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
