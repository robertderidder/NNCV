{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from torch.optim import AdamW\n",
    "from torch.optim import lr_scheduler\n",
    "from model import Model\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import Cityscapes, wrap_dataset_for_transforms_v2\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.transforms.v2 import (\n",
    "    Compose,\n",
    "    Normalize,\n",
    "    Resize,\n",
    "    ToImage,\n",
    "    ToDtype,\n",
    "    RandomHorizontalFlip,\n",
    "    RandomVerticalFlip,\n",
    ")\n",
    "\n",
    "# Mapping class IDs to train IDs\n",
    "id_to_trainid = {cls.id: cls.train_id for cls in Cityscapes.classes}\n",
    "def convert_to_train_id(label_img: torch.Tensor) -> torch.Tensor:\n",
    "    return label_img.apply_(lambda x: id_to_trainid[x])\n",
    "\n",
    "# Mapping train IDs to color\n",
    "train_id_to_color = {cls.train_id: cls.color for cls in Cityscapes.classes if cls.train_id != 255}\n",
    "train_id_to_color[255] = (0, 0, 0)  # Assign black to ignored labels\n",
    "\n",
    "def convert_train_id_to_color(prediction: torch.Tensor) -> torch.Tensor:\n",
    "    batch, _, height, width = prediction.shape\n",
    "    color_image = torch.zeros((batch, 3, height, width), dtype=torch.uint8)\n",
    "\n",
    "    for train_id, color in train_id_to_color.items():\n",
    "        mask = prediction[:, 0] == train_id\n",
    "\n",
    "        for i in range(3):\n",
    "            color_image[:, i][mask] = color[i]\n",
    "\n",
    "    return color_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducability\n",
    "# If you add other sources of randomness (NumPy, Random), \n",
    "# make sure to set their seeds as well\n",
    "torch.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "# Define the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transforms to apply to the data\n",
    "class PaintingByNumbersTransform:\n",
    "      def __init__(self, id_to_color=None):\n",
    "          self.id_to_color = id_to_color  # Dictionary mapping class IDs to colors\n",
    "  \n",
    "      def random_recolor(self, label_img):\n",
    "          \"\"\"Assigns random colors to segmentation labels.\"\"\"\n",
    "          h, w = label_img.shape[1:]\n",
    "          recolored = torch.zeros((3, h, w), dtype=torch.uint8)  # Create an empty RGB image\n",
    "                  \n",
    "          unique_labels = label_img.unique()\n",
    "          color_map = {label.item(): torch.randint(0, 256, (3,), dtype=torch.uint8) for label in unique_labels}\n",
    "\n",
    "          for label, color in color_map.items():\n",
    "              mask = (label_img[0] == label)  # label_img shape is [1, h, w]\n",
    "              for c in range(3):\n",
    "                  recolored[c][mask] = color[c]\n",
    "             \n",
    "          return recolored\n",
    "  \n",
    "      def __call__(self, img, target):\n",
    "          if torch.rand(1).item() > 0.5:\n",
    "              # Load the actual ground truth color image\n",
    "              gt_color = self.random_recolor(target)\n",
    "  \n",
    "              # Blend image and color segmentation map\n",
    "              alpha = torch.rand(1).item() * 0.29 + 0.7  # Random alpha between 0.7 and 0.99\n",
    "              blended_img = alpha * img + (1 - alpha) * gt_color.float() / 255.0\n",
    "              return blended_img, target\n",
    "          \n",
    "          return img, target  # If not applying transformation, return original\n",
    "\n",
    "\n",
    "transform = Compose([\n",
    "    ToImage(),\n",
    "    Resize((256, 256)),\n",
    "    ToDtype(torch.float32, scale=True),\n",
    "    Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    PaintingByNumbersTransform(),\n",
    "    RandomHorizontalFlip(p=0.5),\n",
    "    RandomVerticalFlip(p=0.5),\n",
    "])\n",
    "\n",
    "# Load the dataset and make a split for training and validation\n",
    "train_dataset = Cityscapes(\n",
    "    \"data/cityscapes\", \n",
    "    split=\"train\", \n",
    "    mode=\"fine\", \n",
    "    target_type=\"semantic\", \n",
    "    transforms=transform\n",
    ")\n",
    "valid_dataset = Cityscapes(\n",
    "    \"data/cityscapes\", \n",
    "    split=\"val\", \n",
    "    mode=\"fine\", \n",
    "    target_type=\"semantic\", \n",
    "    transforms=transform\n",
    ")\n",
    "\n",
    "train_dataset = wrap_dataset_for_transforms_v2(train_dataset)\n",
    "valid_dataset = wrap_dataset_for_transforms_v2(valid_dataset)\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset, \n",
    "    batch_size=64, \n",
    "    shuffle=True,\n",
    "    num_workers=10\n",
    ")\n",
    "valid_dataloader = DataLoader(\n",
    "    valid_dataset, \n",
    "    batch_size=64, \n",
    "    shuffle=False,\n",
    "    num_workers=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the loss function\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=255)  # Ignore the void class\n",
    "\n",
    "# Define the optimizer\n",
    "lr1 = 0.01\n",
    "lr2 = 0.001\n",
    "\n",
    "model = Model().to(device)\n",
    "\n",
    "optimizer1 = AdamW([\n",
    "{\"params\": model.model.classifier.parameters(), \"lr\": lr1}  # Higher LR for classifier\n",
    "])\n",
    "\n",
    "optimizer2 = AdamW([\n",
    "{\"params\": model.model.backbone.parameters(), \"lr\": lr2}  # Lower LR for backbone\n",
    "])\n",
    "\n",
    "scheduler = lr_scheduler.StepLR(optimizer1, 2, gamma=0.5, last_epoch=-1)\n",
    "\n",
    "\n",
    "\n",
    "for param in model.model.backbone.parameters():\n",
    "    param.requires_grad = True  # Unfreeze the backbone\n",
    "    \n",
    "for param in model.model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0001/0010\n",
      "Epoch 0002/0010\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 34\u001b[0m\n\u001b[1;32m     32\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmodel(images)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mout\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     33\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 34\u001b[0m losses\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     37\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39margmax(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "best_valid_loss = float('inf')\n",
    "current_best_model_path = None\n",
    "for epoch in range(10):\n",
    "    print(f\"Epoch {epoch+1:04}/{10:04}\")\n",
    "\n",
    "    # Training\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(train_dataloader):\n",
    "\n",
    "        labels = convert_to_train_id(labels)  # Convert class IDs to train IDs\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        labels = labels.long().squeeze(1)  # Remove channel dimension\n",
    "\n",
    "        optimizer1.zero_grad()\n",
    "        outputs = model.model(images)['out']\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer1.step()\n",
    "        \n",
    "    # Validation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        losses = []\n",
    "        for i, (images, labels) in enumerate(valid_dataloader):\n",
    "            labels = convert_to_train_id(labels)  # Convert class IDs to train IDs\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            labels = labels.long().squeeze(1)  # Remove channel dimension\n",
    "\n",
    "            outputs = model.model(images)['out']\n",
    "            loss = criterion(outputs, labels)\n",
    "            losses.append(loss.item())\n",
    "        \n",
    "            if i == 0:\n",
    "                predictions = outputs.softmax(1).argmax(1)\n",
    "\n",
    "                predictions = predictions.unsqueeze(1)\n",
    "                labels = labels.unsqueeze(1)\n",
    "\n",
    "                predictions = convert_train_id_to_color(predictions)\n",
    "                labels = convert_train_id_to_color(labels)\n",
    "\n",
    "                predictions_img = make_grid(predictions.cpu(), nrow=8)\n",
    "                labels_img = make_grid(labels.cpu(), nrow=8)\n",
    "\n",
    "                predictions_img = predictions_img.permute(1, 2, 0).numpy()\n",
    "                labels_img = labels_img.permute(1, 2, 0).numpy()\n",
    "\n",
    "        \n",
    "        valid_loss = sum(losses) / len(losses)\n",
    "       \n",
    "        # if valid_loss < best_valid_loss:\n",
    "        #     best_valid_loss = valid_loss\n",
    "        #     if current_best_model_path:\n",
    "        #         os.remove(current_best_model_path)\n",
    "        #     current_best_model_path = os.path.join(\n",
    "        #         output_dir, \n",
    "        #         f\"best_model-epoch={epoch:04}-val_loss={valid_loss:04}.pth\"\n",
    "        #     )\n",
    "        #     torch.save(model.state_dict(), current_best_model_path)\n",
    "    \n",
    "print(\"Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"model_from_notebook.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diceloss import DiceLoss\n",
    "# Get one batch from the dataloader\n",
    "model.eval()  # No dropout or batchnorm updates\n",
    "with torch.no_grad():\n",
    "    for images, labels in train_dataloader:\n",
    "        labels = convert_to_train_id(labels)\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        labels = labels.long().squeeze(1)\n",
    "        \n",
    "        outputs = model.model(images)['out']\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "    'images': images.cpu(),\n",
    "    'labels': labels.cpu(),\n",
    "    'outputs': outputs.cpu()\n",
    "}, 'batch_dump.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
